Machine Learning Algorithms Maacchhiinnee LLeeaarrnniinngg Allggoorriitthhmss fffoorrr Clllaassssiiififififificcaattiiioonn 
Rob Schapire 
Princeton University

Maacchhiinnee LLeeaarrnniinngg 
• studies how to automatically learn to make accurate 
predictions based on past observations 
• classification problems: 
• classify examples into given set of categories 
new 
example 
machine learning 
algorithm 
classification 
predicted 
rule 
classification 
examples 
training 
labeled

Examples of ClassiExxaampplleess ooff Cllaassssiififififificcaattiiioonn Prroobblleemss 
• text categorization (e.g., spam filtering) 
• fraud detection 
• optical character recognition 
• machine vision (e.g., face detection) 
• natural-language processing 
(e.g., spoken language understanding) 
• market segmentation 
(e.g.: predict if customer will respond to promotion) 
• bioinformatics 
(e.g., classify proteins according to their function) 
...

Characteristics of Modern Chhaarraacctteerriissttiiccss ooff Mooddeerrnn Maacchhiinnee LLeeaarrrnniiinngg 
• primary goal: highly accurate predictions on test data 
• goal is not to uncover underlying “truth” 
• methods should be general purpose, fully automatic and 
“off-the-shelf” 
• however, in practice, incorporation of prior, human 
knowledge is crucial 
• rich interplay between theory and practice 
• emphasis on methods that can handle large datasets

Why Whhyy Ussee Maacchhiinnee LLeeaarrnniinngg?? 
• advantages: 
• often much more accurate than human-crafted rules 
(since data driven) 
• humans often incapable of expressing what they know 
(e.g., rules of English, or how to recognize letters), 
but can easily classify examples 
• don’t need a human expert or programmer 
• automatic method to search for hypotheses explaining 
data 
• cheap and flexible — can apply to any learning task 
• disadvantages 
• need a lot of labeled data 
• error prone — usually impossible to get perfect accuracy

Thhiiss Taallkk 
• machine learning algorithms: 
• decision trees 
• conditions for successful learning 
• boosting 
• support-vector machines 
• others not covered: 
• neural networks 
• nearest neighbor algorithms 
• Naive Bayes 
• bagging 
• random forests 
... 
• practicalities of using machine learning algorithms

Deecciiissiiioonn Trreeeess

Example: Exxaamppllee:: Goooodd vveerrrssuuss Evviill 
• problem: identify people as good or bad from their appearance 
sex mask cape tie ears smokes class 
training data 
batman male yes yes no yes no Good 
robin male yes yes no no no Good 
alfred male no no yes no no Good 
penguin male no no yes no yes Bad 
catwoman female yes no no yes no Bad 
joker male no no no no no Bad 
test data 
batgirl female yes yes no yes no ?? 
riddler male yes no no no no ??

A Decision Deecciissiioonn Trreeee Cllaassssiifififififieerr 
tie 
good 
no yes 
no yes 
bad bad 
cape smokes 
no yes 
good

How Hoow ttoo Buuiiillldd Deecciiissiiioonn Trreeeess 
• choose rule to split on 
• divide data using splitting rule into disjoint subsets 
tie 
no yes 
alfred 
catwoman 
joker 
penguin 
robin 
batman 
batman alfred 
robin 
joker 
catwoman 
penguin

How Hoow ttoo Buuiiillldd Deecciiissiiioonn Trreeeess 
• choose rule to split on 
• divide data using splitting rule into disjoint subsets 
• repeat recursively for each subset 
• stop when leaves are (almost) “pure” 
tie 
no yes 
alfred 
catwoman 
joker 
penguin 
robin 
batman 
batman alfred 
robin 
joker 
catwoman 
penguin
) 
tie 
no yes

How to Choose Hoow ttoo Chhoooossee tthhee Spplliittttiinngg Ruullee 
• key problem: choosing best rule to split on: 
tie 
no yes 
alfred 
catwoman 
joker 
penguin 
robin 
batman 
batman alfred 
robin 
joker 
catwoman 
penguin 
no yes 
alfred 
catwoman 
joker 
penguin 
robin 
batman 
joker 
catwoman 
cape 
batman 
robin 
alfred 
penguin

How to Choose Hoow ttoo Chhoooossee tthhee Spplliittttiinngg Ruullee 
• key problem: choosing best rule to split on: 
tie 
no yes 
alfred 
catwoman 
joker 
penguin 
robin 
batman 
batman alfred 
robin 
joker 
catwoman 
penguin 
no yes 
alfred 
catwoman 
joker 
penguin 
robin 
batman 
joker 
catwoman 
cape 
batman 
robin 
alfred 
penguin 
• idea: choose rule that leads to greatest increase in “purity”

Hoow ttoo Meeaassuurrree Puurriittyy 
• want (im)purity function to look like this: 
(p = fraction of positive examples) 
impurity 
0 
1/2 
1 
p 
• commonly used impurity measures: 
• entropy: .p ln p . (1 . p) ln(1 . p) 
• Gini index: p(1 . p)

Kiinnddss ooff Errrroorr Raatteess 
• training error = fraction of training examples misclassified 
• test error = fraction of test examples misclassified 
• generalization error = probability of misclassifying new 
random example

A Poossssiibbllee Cllaassssiifififififieerr 
cape 
ears 
tie cape 
ears 
good 
good bad 
good bad 
bad 
bad good bad good 
sex 
smokes 
smokes 
no yes 
no yes 
no yes 
no 
no no 
no 
no 
yes 
yes yes yes 
male female yes 
mask 
• perfectly classifies training data 
• BUT: intuitively, overly complex

Another Annootthheerr Poossssiibbllee Clllaassssiiifififififieerrr 
bad good 
no yes 
mask 
• overly simple 
• doesn’t even fit available data

Tree Trreeee Siizzee vveerrssuuss Accccuurrraaccyy 
0.5 
0.55 
0.6 
0.65 
0.7 
0.75 
0.8 
0.85 
0.9 
0 10 20 30 40 50 60 70 80 90 100 
Accuracy 
On training data 
On test data 
.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 
.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 
.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 
.. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. 
                               
                               
                               
                               
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
          
          
          
          
          
          
          
          
          
          
          
          
          
          
                              
40 
20 
30 
error (%) 
50 
tree size 
50 
test
train 
0 100 
10 
• trees must be big enough to fit training data 
(so that “true” patterns are fully captured) 
• BUT: trees that are too big may overfit 
(capture noise or spurious patterns in the data) 
• significant problem: can’t tell best tree size from training error

Ovveerrfififififittttiinngg Exxaamppllee 
• fitting points with a polynomial 
underfit ideal fit overfit 
(degree = 1) (degree = 3) (degree = 20)

Building Buuiillddiinngg aann Accccuurraattee Clllaassssiiifififififieerrr 
• for good test peformance, need: 
• enough training examples 
• good performance on training set 
• classifier that is not too “complex” (“Occam’s razor”) 
• classifiers should be “as simple as possible, but no simpler” 
• “simplicity” closely related to prior expectations

Building Buuiillddiinngg aann Accccuurraattee Clllaassssiiifififififieerrr 
• for good test peformance, need: 
• enough training examples 
• good performance on training set 
• classifier that is not too “complex” (“Occam’s razor”) 
• classifiers should be “as simple as possible, but no simpler” 
• “simplicity” closely related to prior expectations 
• measure “complexity” by: 
• number bits needed to write down 
• number of parameters 
• VC-dimension

Exxaamppllee 
Training data:

Goooodd aanndd Baadd Clllaassssiiifififififieerrrss 
Good: 
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 
	 	 	 	 	 	 	 	 	 	 	 	 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
sufficient data 
low training error 
simple classifier 
Bad: 
               
               
               
               
               
               
               
               
               
               

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
               
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
insufficient data training error classifier 
too high too complex

Thheeoorryy 
• can prove: 
(generalization error)  (training error) + .O 
 r
d
m
! 
with high probability 
• d = VC-dimension 
• m = number training examples

Coonnttrroolllliinngg Trreeee Siizzee 
• typical approach: build very large tree that fully fits training 
data, then prune back 
• pruning strategies: 
• grow on just part of training data, then find pruning with 
minimum error on held out part 
• find pruning that minimizes 
(training error) + constant · (tree size)

Deecciissiioonn Trreeeess 
• best known: 
• C4.5 (Quinlan) 
• CART (Breiman, Friedman, Olshen & Stone) 
• very fast to train and evaluate 
• relatively easy to interpret 
• but: accuracy often not state-of-the-art

Boooossttiinngg

Example: Exxaamppllee:: Sppaam Fiiillltteerrriiinngg 
• problem: filter out spam (junk email) 
• gather large collection of examples of spam and non-spam: 
From: yoav@att.com Rob, can you review a paper... non-spam 
From: xa412@hotmail.com Earn money without working!!!! ... spam 
...
...... 
• goal: have computer learn from examples to distinguish spam 
from non-spam

Example: Exxaamppllee:: Sppaam Fiiillltteerrriiinngg 
• problem: filter out spam (junk email) 
• gather large collection of examples of spam and non-spam: 
From: yoav@att.com Rob, can you review a paper... non-spam 
From: xa412@hotmail.com Earn money without working!!!! ... spam 
...
...... 
• goal: have computer learn from examples to distinguish spam 
from non-spam 
• main observation: 
• easy to find “rules of thumb” that are “often” correct 
• If ‘v1agr@’ occurs in message, then predict ‘spam’ 
• hard to find single rule that is very highly accurate

Thhee Boooossttiinngg Apppprrooaacchh 
• devise computer program for deriving rough rules of thumb 
• apply procedure to subset of emails 
• obtain rule of thumb 
• apply to 2nd subset of emails 
• obtain 2nd rule of thumb 
• repeat T times

Deettaaiillss 
• how to choose examples on each round? 
• concentrate on “hardest” examples 
(those most often misclassified by previous rules of 
thumb) 
• how to combine rules of thumb into single prediction rule? 
• take (weighted) majority vote of rules of thumb

Boooossttiinngg 
• boosting = general method of converting rough rules of 
thumb into highly accurate prediction rule 
• technically: 
• assume given “weak” learning algorithm that can 
consistently find classifiers (“rules of thumb”) at least 
slightly better than random, say, accuracy  55% 
• given sufficient data, a boosting algorithm can provably 
construct single classifier with very high accuracy, say, 
99%

AddaaBoooosstt 
• given training examples (xi , yi ) where yi 2 {.1,+1}

AddaaBoooosstt 
• given training examples (xi , yi ) where yi 2 {.1,+1} 
• for t = 1, . . . ,T: 
• train weak classifier (“rule of thumb”) ht on Dt

AddaaBoooosstt 
• given training examples (xi , yi ) where yi 2 {.1,+1} 
• initialize D1 = uniform distribution on training examples 
• for t = 1, . . . ,T: 
• train weak classifier (“rule of thumb”) ht on Dt

AddaaBoooosstt 
• given training examples (xi , yi ) where yi 2 {.1,+1} 
• initialize D1 = uniform distribution on training examples 
• for t = 1, . . . ,T: 
• train weak classifier (“rule of thumb”) ht on Dt 
• choose t > 0 
• compute new distribution Dt+1: 
• for each example i : 
multiply Dt(xi ) by 
 
e.t (< 1) if yi = ht (xi ) 
et (> 1) if yi 6= ht (xi ) 
• renormalize

AddaaBoooosstt 
• given training examples (xi , yi ) where yi 2 {.1,+1} 
• initialize D1 = uniform distribution on training examples 
• for t = 1, . . . ,T: 
• train weak classifier (“rule of thumb”) ht on Dt 
• choose t > 0 
• compute new distribution Dt+1: 
• for each example i : 
multiply Dt(xi ) by 
 
e.t (< 1) if yi = ht (xi ) 
et (> 1) if yi 6= ht (xi ) 
• renormalize 
• output final classifier Hfinal(x) = sign
 
X
t 
tht(x)
!

Tooyy Exxaamppllee 
D1 
weak classifiers = vertical or horizontal half-planes

Roouunndd 11
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
h1 
a
e1
1
=0.30 
=0.42 
D2

Roouunndd 22
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
! ! ! ! 
a
e2
2
=0.21 
=0.65 
h2 D3

Roouunndd 33 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
" " " " 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
# # # # 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
$ $ $ $ $ $ $ $ $ $ $ $ $ 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
% % % % % % % % % % % % % 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
& & & & 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
' ' ' ' 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
( ( ( ( ( ( ( ( ( ( ( ( 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
) ) ) ) ) ) ) ) ) ) ) ) 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
* * * * * * * * * * * * * * * * 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
+ + + + + + + + + + + + + + + + 
, , , , , , , , , , , , , , , , 
, , , , , , , , , , , , , , , , 
, , , , , , , , , , , , , , , , 
, , , , , , , , , , , , , , , , 
, , , , , , , , , , , , , , , , 
, , , , , , , , , , , , , , , , 
, , , , , , , , , , , , , , , , 
, , , , , , , , , , , , , , , , 
, , , , , , , , , , , , , , , , 
, , , , , , , , , , , , , , , , 
- - - - - - - - - - - - - - - - 
- - - - - - - - - - - - - - - - 
- - - - - - - - - - - - - - - - 
- - - - - - - - - - - - - - - - 
- - - - - - - - - - - - - - - - 
- - - - - - - - - - - - - - - - 
- - - - - - - - - - - - - - - - 
- - - - - - - - - - - - - - - - 
- - - - - - - - - - - - - - - - 
- - - - - - - - - - - - - - - - 
h3 
a
e3
3=0.92 
=0.14

Fiinnaall Clllaassssiiifififififieerrr 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
. . . . . . . . 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
/ / / / / / / / 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
0 0 0 0 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
1 1 1 1 
2 2 2 2 2 2 2 2 2 2 2 
2 2 2 2 2 2 2 2 2 2 2 
2 2 2 2 2 2 2 2 2 2 2 
2 2 2 2 2 2 2 2 2 2 2 
2 2 2 2 2 2 2 2 2 2 2 
2 2 2 2 2 2 2 2 2 2 2 
2 2 2 2 2 2 2 2 2 2 2 
3 3 3 3 3 3 3 3 3 3 3 
3 3 3 3 3 3 3 3 3 3 3 
3 3 3 3 3 3 3 3 3 3 3 
3 3 3 3 3 3 3 3 3 3 3 
3 3 3 3 3 3 3 3 3 3 3 
3 3 3 3 3 3 3 3 3 3 3 
3 3 3 3 3 3 3 3 3 3 3 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
4 4 4 4 4 4 4 4 4 4 4 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
5 5 5 5 5 5 5 5 5 5 5 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
6 6 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
7 7 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
8 8 8 8 8 8 8 8 8 8 8 8 8 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
9 9 9 9 9 9 9 9 9 9 9 9 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
: : : : : : : : : 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
; ; ; ; ; ; ; ; ; 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
< < 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
= = 
> > > > > > > > > > > > 
> > > > > > > > > > > > 
> > > > > > > > > > > > 
> > > > > > > > > > > > 
> > > > > > > > > > > > 
> > > > > > > > > > > > 
> > > > > > > > > > > > 
> > > > > > > > > > > > 
> > > > > > > > > > > > 
> > > > > > > > > > > > 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
? ? ? ? ? ? ? ? ? ? ? ? 
H
final 
=sign 0.42 + 0.65 + 0.92 
=

Thheeoorryy:: Trraaiinniinngg Errrrrroorrr 
• weak learning assumption: each weak classifier at least slightly 
better than random 
• i.e., (error of ht on Dt)  1/2 . 
 for some 
 > 0 
• given this assumption, can prove: 
training error(Hfinal)  e.2

2T

How Will Test Error Hoow Wiillll Teesstt Errrroorr Beehhaavvee?? ((A Fiiirrrsstt Guueessss)) 
20 40 60 80 100 
0.2 
0.4 
0.6 
0.8
1 
# of rounds ( 
error 
T) 
train 
test 
expect: 
• training error to continue to drop (or reach zero) 
• test error to increase when Hfinal becomes “too complex” 
• “Occam’s razor” 
• overfitting 
• hard to know when to stop training

Accttuuaall Tyyppiiccaall Ruunn 
10 100 1000 
0
5 
10 
15 
20 
# of rounds (T 
C4.5 test error 
) 
train
test 
error 
(boosting C4.5 on 
“letter” dataset) 
• test error does not increase, even after 1000 rounds 
• (total size > 2,000,000 nodes) 
• test error continues to drop even after training error is zero! 
# rounds 
5 100 1000 
train error 0.0 0.0 0.0 
test error 8.4 3.3 3.1 
• Occam’s razor wrongly predicts “simpler” rule is better

The Thhee Maarrggiinnss Exxppllaannaattiioonn 
• key idea: 
• training error only measures whether classifications are 
right or wrong 
• should also consider confidence of classifications

The Thhee Maarrggiinnss Exxppllaannaattiioonn 
• key idea: 
• training error only measures whether classifications are 
right or wrong 
• should also consider confidence of classifications 
• recall: Hfinal is weighted majority vote of weak classifiers

The Thhee Maarrggiinnss Exxppllaannaattiioonn 
• key idea: 
• training error only measures whether classifications are 
right or wrong 
• should also consider confidence of classifications 
• recall: Hfinal is weighted majority vote of weak classifiers 
• measure confidence by margin = strength of the vote 
• empirical evidence and mathematical proof that: 
• large margins ) better generalization error 
(regardless of number of rounds) 
• boosting tends to increase margins of training examples 
(given weak learning assumption)

Application: Human-computer Apppplliiccaattiioonn:: Huumaann---ccoomppuutteerrr Sppookkeenn Diiaalloogguuee 
[with Rahim, Di Fabbrizio, Dutton, Gupta, Hollister & Riccardi] 
• application: automatic “store front” or “help desk” for AT&T 
Labs’ Natural Voices business 
• caller can request demo, pricing information, technical 
support, sales agent, etc. 
• interactive dialogue

Hoow IItt Woorrkkss 
speech 
computer 
utterance 
understanding 
natural language 
text response 
text 
raw
recognizer 
speech 
text.to.speech automatic 
category 
predicted 
Human 
manager 
dialogue 
• NLU’s job: classify caller utterances into 24 categories 
(demo, sales rep, pricing info, yes, no, etc.) 
• weak classifiers: test for presence of word or phrase

Application: Apppplliiccaattiioonn:: Deetteeccttiiinngg Faacceess 
[Viola & Jones] 
• problem: find faces in photograph or movie 
• weak classifiers: detect light/dark rectangles in image 
• many clever tricks to make extremely fast and accurate

Boooossttiinngg 
• fast (but not quite as fast as other methods) 
• simple and easy to program 
• flexible: can combine with any learning algorithm, e.g. 
• C4.5 
• very simple rules of thumb 
• provable guarantees 
• state-of-the-art accuracy 
• tends not to overfit (but occasionally does) 
• many applications

Support-Vector Suuppppoorrtt--Veeccttoorr Maacchhiiinneess

Geeoomeettrryy ooff SVM’’ss 
• given linearly separable data

Geeoomeettrryy ooff SVM’’ss 
d 
d 
• given linearly separable data 
• margin = distance to separating hyperplane 
• choose hyperplane that maximizes minimum margin 
• intuitively: 
• want to separate +’s from .’s as much as possible 
• margin = measure of confidence

Theoretical Thheeoorreettiiccaall JJuussttiififififificcaattiioonn 
• let  = minimum margin 
R = radius of enclosing sphere 
• then 
VC-dim  

R
 
2 
• so larger margins ) lower “complexity” 
• independent of number of dimensions 
• in contrast, unconstrained hyperplanes in Rn have 
VC-dim = (# parameters) = n + 1

Finding the Maximum Fiinnddiinngg tthhee Maaxxiiimuum Maarrrggiiinn Hyyppeerrppllaannee 
• examples xi , yi where xi 2 Rn, yi 2 {.1,+1} 
• find hyperplane v · x = 0 with kvk= 1

Finding the Maximum Fiinnddiinngg tthhee Maaxxiiimuum Maarrrggiiinn Hyyppeerrppllaannee 
• examples xi , yi where xi 2 Rn, yi 2 {.1,+1} 
• find hyperplane v · x = 0 with kvk= 1 
• margin = y(v · x) 
• maximize:  
subject to: yi (v · xi )   and kvk= 1

Finding the Maximum Fiinnddiinngg tthhee Maaxxiiimuum Maarrrggiiinn Hyyppeerrppllaannee 
• examples xi , yi where xi 2 Rn, yi 2 {.1,+1} 
• find hyperplane v · x = 0 with kvk= 1 
• margin = y(v · x) 
• maximize:  
subject to: yi (v · xi )   and kvk= 1 
• set w = v/ ) kwk= 1/

Finding the Maximum Fiinnddiinngg tthhee Maaxxiiimuum Maarrrggiiinn Hyyppeerrppllaannee 
• examples xi , yi where xi 2 Rn, yi 2 {.1,+1} 
• find hyperplane v · x = 0 with kvk= 1 
• margin = y(v · x) 
• maximize:  
subject to: yi (v · xi )   and kvk= 1 
• set w = v/ ) kwk= 1/ 
• minimize: 1
2 kwk2 
subject to: yi (w · xi )  1

Coonnvveexx Duuaalll 
• form Lagrangian, set @/@w = 0 
• w =
X
i 
iyixi 
• get quadratic program: 
• maximize
X
i 
i . 1
2
X
i ,j 
ijyiyjxi · xj 
subject to: i  0 
• i = Lagrange multiplier 
> 0 ) support vector 
• key points: 
• optimal w is linear combination of support vectors 
• dependence on xi ’s only through inner products 
• maximization problem is convex with no local maxima

What If Not Whhaatt IIff Noott LLiiinneeaarrrlllyy Seeppaarraabbllee?? 
• answer #1: penalize each point by distance from margin 1, 
i.e., minimize: 
1
2 kwk2 +constant ·
X
i 
max{0, 1 . yi (w · xi )} 
• answer #2: map into higher dimensional space in which data 
becomes linearly separable

Exxaamppllee 
• not linearly separable

Exxaamppllee 
• not linearly separable 
• map x = (x1, x2) 7! (x) = (1, x1, x2, x1x2, x2
1 , x2
2 )

Exxaamppllee 
• not linearly separable 
• map x = (x1, x2) 7! (x) = (1, x1, x2, x1x2, x2
1 , x2
2 ) 
• hyperplane in mapped space has form 
a + bx1 + cx2 + dx1x2 + ex2
1 + fx2
2 = 0 
= conic in original space 
• linearly separable in mapped space

Why Mapping to Whhyy Maappppiiinngg ttoo Hiigghh Diimeennssiioonnss IIIss Duumbb 
• can carry idea further 
• e.g., add all terms up to degree d 
• then n dimensions mapped to O(nd ) dimensions 
• huge blow-up in dimensionality

Why Mapping to Whhyy Maappppiiinngg ttoo Hiigghh Diimeennssiioonnss IIIss Duumbb 
• can carry idea further 
• e.g., add all terms up to degree d 
• then n dimensions mapped to O(nd ) dimensions 
• huge blow-up in dimensionality 
• statistical problem: amount of data needed often proportional 
to number of dimensions (“curse of dimensionality”) 
• computational problem: very expensive in time and memory 
to work in high dimensions

How Hoow SVM’’ss Avvooiidd Bootthh Prroobblleemss 
• statistically, may not hurt since VC-dimension independent of 
number of dimensions ((R/)2) 
• computationally, only need to be able to compute inner 
products 
(x) · (z) 
• sometimes can do very efficiently using kernels

Exxaamppllee ((ccoonnttiiinnuueedd)) 
• modify  slightly: 
x = (x1, x2) 7! (x) = (1, x1, x2, x1x2, x2
1 , x2
2 )

Exxaamppllee ((ccoonnttiiinnuueedd)) 
• modify  slightly: 
x = (x1, x2) 7! (x) = (1,p2x1,p2x2,p2x1x2, x2
1 , x2
2 )

Exxaamppllee ((ccoonnttiiinnuueedd)) 
• modify  slightly: 
x = (x1, x2) 7! (x) = (1,p2x1,p2x2,p2x1x2, x2
1 , x2
2 ) 
• then 
(x) · (z) = 1 + 2x1z1 + 2x2z2 + 2x1x2z1z2 + x2
1 z2
1 + x2
2 z2
2 
= (1 + x1z1 + x2z2)2 
= (1 + x · z)2 
• simply use in place of usual inner product

Exxaamppllee ((ccoonnttiiinnuueedd)) 
• modify  slightly: 
x = (x1, x2) 7! (x) = (1,p2x1,p2x2,p2x1x2, x2
1 , x2
2 ) 
• then 
(x) · (z) = 1 + 2x1z1 + 2x2z2 + 2x1x2z1z2 + x2
1 z2
1 + x2
2 z2
2 
= (1 + x1z1 + x2z2)2 
= (1 + x · z)2 
• simply use in place of usual inner product 
• in general, for polynomial of degree d, use (1 + x · z)d 
• very efficient, even though finding hyperplane in O(nd ) 
dimensions

Keerrnneellss 
• kernel = function K for computing 
K(x, z) = (x) · (z) 
• permits efficient computation of SVM’s in very high 
dimensions 
• K can be any symmetric, positive semi-definite function 
(Mercer’s theorem) 
• some kernels: 
• polynomials 
• Gaussian exp 
..
. kx . zk2 /2
 
• defined over structures (trees, strings, sequences, etc.) 
• evaluation: 
w · (x) =
X
iyi(xi ) · (x) =
X
i yiK(xi , x) 
• time depends on # support vectors

SVM’’ss vveerrssuuss Boooossttiiinngg 
• both are large-margin classifiers 
(although with slightly different definitions of margin) 
• both work in very high dimensional spaces 
(in boosting, dimensions correspond to weak classifiers) 
• but different tricks are used: 
• SVM’s use kernel trick 
• boosting relies on weak learner to select one dimension 
(i.e., weak classifier) to add to combined classifier

Application: Apppplliiccaattiioonn:: Teexxtt Caatteeggoorriizzaattiioonn 
[Joachims] 
• goal: classify text documents 
• e.g.: spam filtering 
• e.g.: categorize news articles by topic 
• need to represent text documents as vectors in Rn: 
• one dimension for each word in vocabulary 
• value = # times word occurred in particular document 
• (many variations) 
• kernels don’t help much 
• performance state of the art

Application: Recognizing Handwritten Apppplliiccaattiioonn:: Reeccooggnniiizziiinngg Haannddwrriitttteenn Chhaarraacctteerrss 
[Cortes & Vapnik] 
• examples are 16 . 16 pixel images, viewed as vectors in R256 
7 7 4 8 0 1 4 
• kernels help: degree error dimensions 
1 12.0 256 
2 4.7  33000 
3 4.4  106 
4 4.3  109 
5 4.3  1012 
6 4.2  1014 
7 4.3  1016 
human 2.5 
• to choose best degree: 
• train SVM for each degree 
• choose one with minimum VC-dimension  (R/)2

SVM’’ss 
• fast algorithms now available, but not so simple to program 
(but good packages available) 
• state-of-the-art accuracy 
• power and flexibility from kernels 
• theoretical justification 
• many applications

Other Machine Otthheerr Maacchhiiinnee LLeeaarrnniinngg Prroobblleem Arrreeaass 
• supervised learning 
• classification 
• regression – predict real-valued labels 
• rare class / cost-sensitive learning 
• unsupervised – no labels 
• clustering 
• density estimation 
• semi-supervised 
• in practice, unlabeled examples much cheaper than 
labeled examples 
• how to take advantage of both labeled and unlabeled 
examples 
• active learning – how to carefully select which unlabeled 
examples to have labeled 
• on-line learning – getting one example at a time

Prraaccttiiccaalliittiieess

Geettttiinngg Daattaa 
• more is more 
• want training data to be like test data 
• use your knowledge of problem to know where to get training 
data, and what to expect test data to be like

Chhoooossiinngg Feeaattuurreess 
• use your knowledge to know what features would be helpful 
for learning 
• redundancy in features is okay, and often helpful 
• most modern algorithms do not require independent 
features 
• too many features? 
• could use feature selection methods 
• usually preferable to use algorithm designed to handle 
large feature sets

Chhoooossiinngg aann Allggoorriitthhm 
• first step: identify appropriate learning paradigm 
• classification? regression? 
• labeled, unlabeled or a mix? 
• class proportions heavily skewed? 
• goal to predict probabilities? rank instances? 
• is interpretability of the results important? 
(keep in mind, no guarantees)

Chhoooossiinngg aann Allggoorriitthhm 
• first step: identify appropriate learning paradigm 
• classification? regression? 
• labeled, unlabeled or a mix? 
• class proportions heavily skewed? 
• goal to predict probabilities? rank instances? 
• is interpretability of the results important? 
(keep in mind, no guarantees) 
• in general, no learning algorithm dominates all others on all 
problems 
• SVM’s and boosting decision trees (as well as other tree 
ensemble methods) seem to be best off-the-shelf 
algorithms 
• even so, for some problems, difference in performance 
among these can be large, and sometimes, much simpler 
methods do better

Choosing Chhoooossiinngg aann Allggoorriitthhm ((ccoonntt..)) 
• sometimes, one particular algorithm seems to naturally fit 
problem, but often, best approach is to try many algorithms 
• use knowledge of problem and algorithms to guide 
decisions 
• e.g., in choice of weak learner, kernel, etc. 
• usually, don’t know what will work until you try 
• be sure to try simple stuff! 
• some packages (e.g. weka) make easy to try many 
algorithms, though implementations are not always 
optimal

Teessttiinngg Peerrffoorrmaannccee 
• does it work? which algorithm is best? 
• train on part of available data, and test on rest 
• if dataset large (say, in 1000’s), can simply set aside 
 1000 random examples as test 
• otherwise, use 10-fold cross validation 
• break dataset randomly into 10 parts 
• in turn, use each block as a test set, training on 
other 9 blocks

Teessttiinngg Peerrffoorrmaannccee 
• does it work? which algorithm is best? 
• train on part of available data, and test on rest 
• if dataset large (say, in 1000’s), can simply set aside 
 1000 random examples as test 
• otherwise, use 10-fold cross validation 
• break dataset randomly into 10 parts 
• in turn, use each block as a test set, training on 
other 9 blocks 
• repeat many times 
• use same train/test splits for each algorithm 
• might be natural split (e.g., train on data from 2004-06, test 
on data from 2007) 
• however, can confound results — bad performance 
because of algorithm, or change of distribution?

Seelleeccttiinngg Paarraameetteerrss 
• sometimes, theory can guide setting of parameters, possibly 
based on statistics measurable on training set 
• other times, need to use trial and test, as before 
• danger: trying too many combinations can lead to overfitting 
test set 
• break data into train, validation and test sets 
• set parameters using validation set 
• measure performance on test set for selected 
parameter settings 
• or do cross-validation within cross-validation 
• trying many parameter settings is also very computationally 
expensive

Ruunnnniinngg Exxppeerrriiimeennttss 
• automate everything! 
• write one script that does everything at the push of a 
single button 
• fewer errors 
• easy to re-run (for instance, if computer crashes in 
middle of experiment) 
• have explicit, scientific record in script of exact 
experiments that were executed

Ruunnnniinngg Exxppeerrriiimeennttss 
• automate everything! 
• write one script that does everything at the push of a 
single button 
• fewer errors 
• easy to re-run (for instance, if computer crashes in 
middle of experiment) 
• have explicit, scientific record in script of exact 
experiments that were executed 
• if running many experiments: 
• put result of each experiment in a separate file 
• use script to scan for next experiment to run based on 
which files have or have not already been created 
• makes very easy to re-start if computer crashes 
• easy to run many experiments in parallel if have multiple 
processors/computers 
• also need script to automatically gather and compile 
results

IIff Wrrriiittiiinngg Yoouurr Ownn Cooddee 
• R and matlab are great for easy coding, but for speed, may 
need C or java 
• debugging machine learning algorithms is very tricky! 
• hard to tell if working, since don’t know what to expect 
• run on small cases where can figure out answer by hand 
• test each module/subroutine separately 
• compare to other implementations 
(written by others, or written in different language) 
• compare to theory or published results

Suummaarryy 
• central issues in machine learning: 
• avoidance of overfitting 
• balance between simplicity and fit to data 
• machine learning algorithms: 
• decision trees 
• boosting 
• SVM’s 
• many not covered 
• looked at practicalities of using machine learning methods 
(will see more in lab)

Further reading on machine learning in general: 
Ethem Alpaydin. Introduction to machine learning. MIT Press, 2004. 
Christopher M. Bishop. Pattern recognition and machine learning. Springer, 2006. 
Richard O. Duda, Peter E. Hart and David G. Stork. Pattern Classification (2nd ed.). 
Wiley, 2000. 
Trevor Hastie, Robert Tibshirani and Jerome Friedman. The Elements of Statistical 
Learning : Data Mining, Inference, and Prediction. Springer, 2001. 
Tom M. Mitchell. Machine Learning. McGraw Hill, 1997. 
Vladimir N. Vapnik. Statistical Learning Theory. Wiley, 1998. 
Decision trees: 
Leo Breiman, Jerome H. Friedman, Richard A. Olshen and Charles J. Stone. Classification 
and Regression Trees. Wadsworth & Brooks, 1984. 
J. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993. 
Boosting: 
Ron Meir and Gunnar R.atsch. An Introduction to Boosting and Leveraging. In Advanced 
Lectures on Machine Learning (LNAI2600), 2003. 
www-ee.technion.ac.il/rmeir/Publications/MeiRae03.pdf 
Robert E. Schapire. The boosting approach to machine learning: An overview. In Nonlinear 
Estimation and Classification, Springer, 2003. 
www.cs.princeton.edu/schapire/boost.html 
Support-vector machines: 
Christopher J. C. Burges. A Tutorial on Support Vector Machines for Pattern Recognition. 
Data Mining and Knowledge Discovery, 2(2):121–167, 1998. 
research.microsoft.com/cburges/papers/SVMTutorial.pdf 
Nello Cristianni and John Shawe-Taylor. An Introduction to Support Vector Machines and 
Other Kernel-based Learning Methods. Cambridge University Press, 2000. 
www.support-vector.net

